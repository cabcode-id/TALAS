{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 593
        },
        "id": "E56bMe2MltWE",
        "outputId": "0a7b5c96-7272-425d-8e89-c2f6044ea06f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "llama-cloud 0.1.13 requires httpx>=0.20.0, but you have httpx 0.13.3 which is incompatible.\n",
            "llama-index-llms-lmstudio 0.2.1 requires llama-index-core<0.12.0,>=0.11.0, but you have llama-index-core 0.12.22 which is incompatible.\n",
            "openai 1.65.4 requires httpx<1,>=0.23.0, but you have httpx 0.13.3 which is incompatible.\n",
            "\n",
            "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# Sentiment Analysis Dependencies\n",
        "!pip install -q contractions scikit-learn Sastrawi googletrans==4.0.0-rc1 langdetect\n",
        "import joblib\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import unicodedata\n",
        "from googletrans import Translator\n",
        "import contractions\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Time Series Dependencies\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import yfinance as yf\n",
        "\n",
        "output_tfidf = 'tfidf_vectorizer.joblib'\n",
        "output_rf = 'random_forest_model.joblib'\n",
        "output_time_series = 'time_series_model.h5'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "35F6wWESuFbU"
      },
      "source": [
        "**Input Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "BC4dAXe6shJF"
      },
      "outputs": [],
      "source": [
        "# sentiment\n",
        "new_text = \"Revision of Subsidized Fertilizer Policy, Now Farmers Can Redeem Using KTP\"\n",
        "\n",
        "# time series\n",
        "stock_symbol = 'FTT-USD' # tambahkan .JK untuk bursa efek indonesia (BBCA.JK) | -USD untuk global\n",
        "start_date = '2022-11-14'\n",
        "end_date = '2023-11-14'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bKNfWPNpcaZ",
        "outputId": "2fba853f-e5ee-4dbd-efe0-139051487847"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[*********************100%***********************]  1 of 1 completed\n",
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 13ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 12ms/step\n",
            "Harga pembukaan aktual terakhir: 2023-11-13 00:00:00 = Ticker\n",
            "FTT-USD    3.425437\n",
            "Name: 2023-11-13 00:00:00, dtype: float64\n",
            "Harga pembukaan peramalan hari pertama: 2023-11-14 00:00:00 = 1.9761346578598022\n",
            "Selisih harga pembukaan saham antara hari aktual terakhir dan hari pertama peramalan: Ticker\n",
            "FTT-USD   -1.449302\n",
            "Name: 2023-11-13 00:00:00, dtype: float64\n",
            "Persentase Perubahan: Ticker\n",
            "FTT-USD   -42.309998\n",
            "Name: 2023-11-13 00:00:00, dtype: float64%\n",
            "\n",
            "Metrik terbobot: Ticker\n",
            "FTT-USD    0.28845\n",
            "Name: 2023-11-13 00:00:00, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# Fungsi-fungsi pra-pemrosesan teks\n",
        "def strip_html_tags(text):\n",
        "    # Fungsi ini menghapus tag HTML dari teks menggunakan BeautifulSoup\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    # Fungsi ini menghapus karakter aksen dari teks menggunakan normalisasi Unicode\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def pre_process_text(text, language):\n",
        "    # Fungsi ini melakukan pra-pemrosesan teks seperti mengonversi teks ke huruf kecil,\n",
        "    # menghapus tag HTML, karakter aksen, kontraksi, dan karakter khusus\n",
        "    text = text.lower()\n",
        "    text = strip_html_tags(text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    text = remove_accented_chars(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I | re.A)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    if language == 'indonesian':\n",
        "        text = preprocess_text_sastrawi(text)\n",
        "    return text\n",
        "\n",
        "# Fungsi pra-pemrosesan teks khusus Bahasa Indonesia\n",
        "def preprocess_text_sastrawi(text):\n",
        "    # Fungsi ini menggunakan Sastrawi untuk menghapus stop word dan melakukan stemming pada teks Bahasa Indonesia\n",
        "    factory1 = StopWordRemoverFactory()\n",
        "    stopword_sastrawi = factory1.create_stop_word_remover()\n",
        "\n",
        "    factory2 = StemmerFactory()\n",
        "    stemmer_sastrawi = factory2.create_stemmer()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stopword_sastrawi.remove(token) for token in tokens]\n",
        "    tokens = [stemmer_sastrawi.stem(token) for token in tokens if token != '']\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Load the models\n",
        "# Memuat model TF-IDF Vectorizer dan Random Forest dari file yang telah diunduh\n",
        "tfidf_vectorizer = joblib.load(output_tfidf)\n",
        "rf_classifier = joblib.load(output_rf)\n",
        "\n",
        "# Pra-pemrosesan teks baru untuk analisis sentimen\n",
        "preprocessed_text = pre_process_text(new_text, 'indonesian')\n",
        "\n",
        "# Mengonversi teks yang telah di-preprocess menjadi fitur TF-IDF\n",
        "new_text_tfidf = tfidf_vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Melakukan prediksi sentimen menggunakan model Random Forest\n",
        "predicted_label = rf_classifier.predict(new_text_tfidf)\n",
        "\n",
        "# Menerjemahkan teks ke bahasa Inggris\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(new_text, dest='en').text\n",
        "\n",
        "# Mengonversi teks yang telah diterjemahkan menjadi fitur TF-IDF\n",
        "translated_text_tfidf = tfidf_vectorizer.transform([translated_text])\n",
        "\n",
        "# Menampilkan prediksi sentimen\n",
        "predicted_sentiment = rf_classifier.predict(translated_text_tfidf)\n",
        "sentiment_probability = rf_classifier.predict_proba(translated_text_tfidf)[0, 1]\n",
        "\n",
        "threshold = 0.5  # Threshold bisa diatur sesuai kebutuhan\n",
        "sentiment = \"Positif\" if sentiment_probability > threshold else \"Negatif\"\n",
        "\n",
        "# Analisis Time Series\n",
        "\n",
        "# Mendapatkan data historis saham\n",
        "new_df = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "\n",
        "# Memilih kolom 'Open' (butuhnya opening price)\n",
        "new_ts = new_df['Open'].values\n",
        "\n",
        "# Normalisasi data\n",
        "scaler = StandardScaler()\n",
        "new_data_normalized = scaler.fit_transform(np.array(new_ts).reshape(-1, 1))\n",
        "\n",
        "# Memastikan data baru dalam format urutan yang serupa dengan data latihan\n",
        "seq_length = 30\n",
        "\n",
        "# Menyiapkan X_new_data\n",
        "X_new_data = []\n",
        "\n",
        "for i in range(len(new_data_normalized) - seq_length):\n",
        "    X_new_data.append(new_data_normalized[i:i + seq_length])\n",
        "\n",
        "# Mengonversi X_new_data menjadi array numpy\n",
        "X_new_data = np.array(X_new_data)\n",
        "\n",
        "# Memuat model analisis time series yang telah dilatih sebelumnya\n",
        "model = load_model(output_time_series)\n",
        "\n",
        "# Melakukan prediksi menggunakan model time series\n",
        "predictions = model.predict(X_new_data)\n",
        "\n",
        "# Peramalan\n",
        "forecast_days = 5\n",
        "X_forecast = np.copy(new_data_normalized[-seq_length:])\n",
        "\n",
        "forecasted_values = []\n",
        "for _ in range(forecast_days):\n",
        "    forecasted_value = model.predict(X_forecast.reshape(1, seq_length, 1))\n",
        "    forecasted_values.append(forecasted_value[0, 0])\n",
        "\n",
        "    X_forecast = np.roll(X_forecast, -1)\n",
        "    X_forecast[-1] = forecasted_value\n",
        "\n",
        "last_actual_day = new_df.index[-1]  # Hari terakhir data aktual\n",
        "forecast_dates = pd.date_range(last_actual_day, periods=forecast_days + 1)[1:]\n",
        "\n",
        "last_actual_opening_price = new_df['Open'].iloc[-1]  # Harga pembukaan hari terakhir pada data aktual\n",
        "first_forecast_opening_price = forecasted_values[0]  # Harga pembukaan hari pertama dalam peramalan\n",
        "\n",
        "price_difference = first_forecast_opening_price - last_actual_opening_price\n",
        "percentage_change = price_difference / last_actual_opening_price\n",
        "\n",
        "print(f\"Harga pembukaan aktual terakhir: {last_actual_day} = {last_actual_opening_price}\")\n",
        "print(f\"Harga pembukaan peramalan hari pertama: {forecast_dates[0]} = {first_forecast_opening_price}\")\n",
        "print(f\"Selisih harga pembukaan saham antara hari aktual terakhir dan hari pertama peramalan: {price_difference}\")\n",
        "print(f\"Persentase Perubahan: {percentage_change*100}%\")\n",
        "\n",
        "# Persentase perubahan disesuaikan menjadi metrik terbobot\n",
        "weighted_metric = (percentage_change + 1) / 2\n",
        "print(f\"\\nMetrik terbobot: {weighted_metric}\")\n",
        "\n",
        "time_series_weight = weighted_metric\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TwAPOgK5nIDz",
        "outputId": "77248178-61fd-41bf-9589-5b15f3ee2a86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bobot: 0.5299575034641106\n",
            "Sentiment: Positive📈\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk mengkombinasikan bobot\n",
        "def combine_weights(sentiment_probability, time_series_weight, sentiment_ratio=0.65):\n",
        "    time_series_ratio = 1 - sentiment_ratio\n",
        "\n",
        "    combined_weight = (sentiment_ratio * sentiment_probability + time_series_ratio * time_series_weight.iloc[-1])\n",
        "    return combined_weight\n",
        "\n",
        "final_weight = combine_weights(sentiment_probability, time_series_weight)\n",
        "final_sentiment = \"Positive📈\" if final_weight > 0.5 else \"Negative📉\"\n",
        "\n",
        "print(\"Bobot:\", final_weight)\n",
        "print(\"Sentiment:\", final_sentiment)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
