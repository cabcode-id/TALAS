{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 252
        },
        "id": "Va5pdk6Jkuyu",
        "outputId": "9f58e066-7853-48c0-bbed-da633c0b0ae4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "[notice] A new release of pip is available: 25.0 -> 25.0.1\n",
            "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\USER\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "!pip install -q contractions scikit-learn Sastrawi googletrans==4.0.0-rc1 langdetect\n",
        "\n",
        "import joblib\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import unicodedata\n",
        "from googletrans import Translator\n",
        "import contractions\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "output_tfidf = 'tfidf_vectorizer.joblib'\n",
        "output_rf = 'random_forest_model.joblib'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PpSjGtmztFqx"
      },
      "source": [
        "**Input Data**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "x22azTNLs3LN"
      },
      "outputs": [],
      "source": [
        "new_text = \"Former FTX executive (yes, FTX, Mas SBF), launched a new crypto currency exchange called Backpack Exchange.This exchange aims to avoid mistakes that cause the fall of FTX by using an independent custody wallet that gives users full control of their funds.Backpack Exchange is looking for an investment of $ 100 million with 10% shares\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1ZwLtTmQkmdG",
        "outputId": "78b425f2-aa5c-48c5-e1f0-3fc28e5944a6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Text: Former FTX executive (yes, FTX, Mas SBF), launched a new crypto currency exchange called Backpack Exchange.This exchange aims to avoid mistakes that cause the fall of FTX by using an independent custody wallet that gives users full control of their funds.Backpack Exchange is looking for an investment of $ 100 million with 10% shares\n",
            "Sentiment Probability: 0.7\n",
            "Sentiment: Positive\n"
          ]
        }
      ],
      "source": [
        "# Fungsi-fungsi pra-pemrosesan teks\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def pre_process_text(text, language):\n",
        "    text = text.lower()\n",
        "    text = strip_html_tags(text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    text = remove_accented_chars(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I | re.A)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    if language == 'indonesian':\n",
        "        text = preprocess_text_sastrawi(text)\n",
        "    return text\n",
        "\n",
        "# Fungsi pra-pemrosesan teks khusus Bahasa Indonesia\n",
        "def preprocess_text_sastrawi(text):\n",
        "    factory1 = StopWordRemoverFactory()\n",
        "    stopword_sastrawi = factory1.create_stop_word_remover()\n",
        "\n",
        "    factory2 = StemmerFactory()\n",
        "    stemmer_sastrawi = factory2.create_stemmer()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stopword_sastrawi.remove(token) for token in tokens]\n",
        "    tokens = [stemmer_sastrawi.stem(token) for token in tokens if token != '']\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Load the models\n",
        "tfidf_vectorizer = joblib.load(output_tfidf)\n",
        "rf_classifier = joblib.load(output_rf)\n",
        "\n",
        "# Preprocess the new text\n",
        "preprocessed_text = pre_process_text(new_text, 'indonesian')\n",
        "\n",
        "# Convert the preprocessed text to TF-IDF features using the loaded tfidf_vectorizer\n",
        "new_text_tfidf = tfidf_vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Predict the label for the new text using the loaded rf_classifier\n",
        "predicted_label = rf_classifier.predict(new_text_tfidf)\n",
        "\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(new_text, dest='en').text\n",
        "\n",
        "# Convert the translated text to TF-IDF features using the loaded tfidf_vectorizer\n",
        "translated_text_tfidf = tfidf_vectorizer.transform([translated_text])\n",
        "\n",
        "# Display the sentiment prediction for the translated text\n",
        "predicted_sentiment = rf_classifier.predict(translated_text_tfidf)\n",
        "sentiment_probability = rf_classifier.predict_proba(translated_text_tfidf)[0, 1]\n",
        "\n",
        "threshold = 0.5  # Threshold bisa diatur sesuai kebutuhan\n",
        "sentiment = \"Positive\" if sentiment_probability > threshold else \"Negative\"\n",
        "\n",
        "# Print hasil prediksi\n",
        "print(\"\\nText:\", translated_text)\n",
        "print(\"Sentiment Probability:\", sentiment_probability)\n",
        "print(\"Sentiment:\", sentiment)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
