{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "E56bMe2MltWE",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 327
        },
        "outputId": "7ccf62b3-5f8c-4cbb-d962-64a9ade55dc6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=134JrTPXdmm6lXZH84ZGk-xsQsEyvvnrL\n",
            "To: /content/tfidf_vectorizer.joblib\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 181k/181k [00:00<00:00, 79.1MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1zhdKOAbGP_wsQRRrhbbxRjQ6Wep_3LKu\n",
            "To: /content/random_forest_model.joblib\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.88M/7.88M [00:00<00:00, 133MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1hQqkpeXQOXH79bNCin1o4So1vGdl3Ent\n",
            "To: /content/time_series_model.h5\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.12M/4.12M [00:00<00:00, 181MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'time_series_model.h5'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 1
        }
      ],
      "source": [
        "# Sentiment Analysis Dependencies\n",
        "!pip install -q contractions scikit-learn Sastrawi googletrans==4.0.0-rc1 langdetect gdown\n",
        "import joblib\n",
        "from google.colab import files\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from bs4 import BeautifulSoup\n",
        "import nltk\n",
        "import re\n",
        "import unicodedata\n",
        "from googletrans import Translator\n",
        "import contractions\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "# Time Series Dependencies\n",
        "import gdown\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import yfinance as yf\n",
        "\n",
        "file_id_tfidf = '134JrTPXdmm6lXZH84ZGk-xsQsEyvvnrL'\n",
        "file_id_rf = '1zhdKOAbGP_wsQRRrhbbxRjQ6Wep_3LKu'\n",
        "file_id_time_series = '1hQqkpeXQOXH79bNCin1o4So1vGdl3Ent'\n",
        "url_tfidf = f'https://drive.google.com/uc?id={file_id_tfidf}'\n",
        "url_rf = f'https://drive.google.com/uc?id={file_id_rf}'\n",
        "url_time_series = f'https://drive.google.com/uc?id={file_id_time_series}'\n",
        "\n",
        "output_tfidf = 'tfidf_vectorizer.joblib'\n",
        "output_rf = 'random_forest_model.joblib'\n",
        "output_time_series = 'time_series_model.h5'\n",
        "\n",
        "gdown.download(url_tfidf, output_tfidf, quiet=False)\n",
        "gdown.download(url_rf, output_rf, quiet=False)\n",
        "gdown.download(url_time_series, output_time_series, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Input Data**"
      ],
      "metadata": {
        "id": "35F6wWESuFbU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# sentiment\n",
        "new_text = \"Revision of Subsidized Fertilizer Policy, Now Farmers Can Redeem Using KTP\"\n",
        "\n",
        "# time series\n",
        "stock_symbol = 'FTT-USD'\n",
        "start_date = '2022-11-14'\n",
        "end_date = '2023-11-14'"
      ],
      "metadata": {
        "id": "BC4dAXe6shJF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi-fungsi pra-pemrosesan teks\n",
        "def strip_html_tags(text):\n",
        "    # Fungsi ini menghapus tag HTML dari teks menggunakan BeautifulSoup\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    # Fungsi ini menghapus karakter aksen dari teks menggunakan normalisasi Unicode\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def pre_process_text(text, language):\n",
        "    # Fungsi ini melakukan pra-pemrosesan teks seperti mengonversi teks ke huruf kecil,\n",
        "    # menghapus tag HTML, karakter aksen, kontraksi, dan karakter khusus\n",
        "    text = text.lower()\n",
        "    text = strip_html_tags(text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    text = remove_accented_chars(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I | re.A)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    if language == 'indonesian':\n",
        "        text = preprocess_text_sastrawi(text)\n",
        "    return text\n",
        "\n",
        "# Fungsi pra-pemrosesan teks khusus Bahasa Indonesia\n",
        "def preprocess_text_sastrawi(text):\n",
        "    # Fungsi ini menggunakan Sastrawi untuk menghapus stop word dan melakukan stemming pada teks Bahasa Indonesia\n",
        "    factory1 = StopWordRemoverFactory()\n",
        "    stopword_sastrawi = factory1.create_stop_word_remover()\n",
        "\n",
        "    factory2 = StemmerFactory()\n",
        "    stemmer_sastrawi = factory2.create_stemmer()\n",
        "\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stopword_sastrawi.remove(token) for token in tokens]\n",
        "    tokens = [stemmer_sastrawi.stem(token) for token in tokens if token != '']\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# Load the models\n",
        "# Memuat model TF-IDF Vectorizer dan Random Forest dari file yang telah diunduh\n",
        "tfidf_vectorizer = joblib.load('/content/tfidf_vectorizer.joblib')\n",
        "rf_classifier = joblib.load('/content/random_forest_model.joblib')\n",
        "\n",
        "# Preprocess the new text\n",
        "# Pra-pemrosesan teks baru untuk analisis sentimen\n",
        "preprocessed_text = pre_process_text(new_text, 'indonesian')\n",
        "\n",
        "# Mengonversi teks yang telah di-preprocess menjadi fitur TF-IDF\n",
        "new_text_tfidf = tfidf_vectorizer.transform([preprocessed_text])\n",
        "\n",
        "# Melakukan prediksi sentimen menggunakan model Random Forest\n",
        "predicted_label = rf_classifier.predict(new_text_tfidf)\n",
        "\n",
        "# Menerjemahkan teks ke bahasa Inggris\n",
        "translator = Translator()\n",
        "translated_text = translator.translate(new_text, dest='en').text\n",
        "\n",
        "# Mengonversi teks yang telah diterjemahkan menjadi fitur TF-IDF\n",
        "translated_text_tfidf = tfidf_vectorizer.transform([translated_text])\n",
        "\n",
        "# Menampilkan prediksi sentimen\n",
        "predicted_sentiment = rf_classifier.predict(translated_text_tfidf)\n",
        "sentiment_probability = rf_classifier.predict_proba(translated_text_tfidf)[0, 1]\n",
        "\n",
        "threshold = 0.5  # Threshold bisa diatur sesuai kebutuhan\n",
        "sentiment = \"Positif\" if sentiment_probability > threshold else \"Negatif\"\n",
        "\n",
        "# Analisis Time Series\n",
        "\n",
        "# Mendapatkan data historis saham\n",
        "new_df = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "\n",
        "# Memilih kolom 'Open' (butuhnya opening price)\n",
        "new_ts = new_df['Open'].values\n",
        "\n",
        "# Normalisasi data\n",
        "scaler = StandardScaler()\n",
        "new_data_normalized = scaler.fit_transform(np.array(new_ts).reshape(-1, 1))\n",
        "\n",
        "# Memastikan data baru dalam format urutan yang serupa dengan data latihan\n",
        "seq_length = 30\n",
        "\n",
        "# Menyiapkan X_new_data\n",
        "X_new_data = []\n",
        "\n",
        "for i in range(len(new_data_normalized) - seq_length):\n",
        "    X_new_data.append(new_data_normalized[i:i + seq_length])\n",
        "\n",
        "# Mengonversi X_new_data menjadi array numpy\n",
        "X_new_data = np.array(X_new_data)\n",
        "\n",
        "# Memuat model analisis time series yang telah dilatih sebelumnya\n",
        "model = load_model('/content/time_series_model.h5')\n",
        "\n",
        "# Melakukan prediksi menggunakan model time series\n",
        "predictions = model.predict(X_new_data)\n",
        "\n",
        "# Peramalan\n",
        "forecast_days = 5\n",
        "X_forecast = np.copy(new_data_normalized[-seq_length:])\n",
        "\n",
        "forecasted_values = []\n",
        "for _ in range(forecast_days):\n",
        "    forecasted_value = model.predict(X_forecast.reshape(1, seq_length, 1))\n",
        "    forecasted_values.append(forecasted_value[0, 0])\n",
        "\n",
        "    X_forecast = np.roll(X_forecast, -1)\n",
        "    X_forecast[-1] = forecasted_value\n",
        "\n",
        "last_actual_day = new_df.index[-1]  # Hari terakhir data aktual\n",
        "forecast_dates = pd.date_range(last_actual_day, periods=forecast_days + 1)[1:]\n",
        "\n",
        "last_actual_opening_price = new_df['Open'][-1]  # Harga pembukaan hari terakhir pada data aktual\n",
        "first_forecast_opening_price = forecasted_values[0]  # Harga pembukaan hari pertama dalam peramalan\n",
        "\n",
        "price_difference = first_forecast_opening_price - last_actual_opening_price\n",
        "percentage_change = price_difference / last_actual_opening_price\n",
        "\n",
        "print(f\"Harga pembukaan aktual terakhir: {last_actual_day} = {last_actual_opening_price}\")\n",
        "print(f\"Harga pembukaan peramalan hari pertama: {forecast_dates[0]} = {first_forecast_opening_price}\")\n",
        "print(f\"Selisih harga pembukaan saham antara hari aktual terakhir dan hari pertama peramalan: {price_difference}\")\n",
        "print(f\"Persentase Perubahan: {percentage_change*100}%\")\n",
        "\n",
        "# Persentase perubahan disesuaikan menjadi metrik terbobot\n",
        "weighted_metric = (percentage_change + 1) / 2\n",
        "print(f\"\\nMetrik terbobot: {weighted_metric}\")\n",
        "\n",
        "time_series_weight = weighted_metric\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bKNfWPNpcaZ",
        "outputId": "3e26f263-6984-4e2f-fdbc-650da3093839"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "11/11 [==============================] - 1s 42ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "Harga pembukaan aktual terakhir: 2023-11-13 00:00:00 = 3.4254369735717773\n",
            "Harga pembukaan peramalan hari pertama: 2023-11-14 00:00:00 = 1.612433671951294\n",
            "Selisih harga pembukaan saham antara hari aktual terakhir dan hari pertama peramalan: -1.8130033016204834\n",
            "Persentase Perubahan: -52.92765027085071%\n",
            "\n",
            "Metrik terbobot: 0.23536174864574644\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Fungsi untuk mengkombinasikan bobot\n",
        "def combine_weights(sentiment_probability, time_series_weight, sentiment_ratio=0.65):\n",
        "    time_series_ratio = 1 - sentiment_ratio\n",
        "\n",
        "    combined_weight = (sentiment_ratio * sentiment_probability + time_series_ratio * time_series_weight)\n",
        "    return combined_weight\n",
        "\n",
        "final_weight = combine_weights(sentiment_probability, time_series_weight)\n",
        "final_sentiment = \"PositiveðŸ“ˆ\" if final_weight > 0.5 else \"NegativeðŸ“‰\"\n",
        "\n",
        "print(\"Bobot:\", final_weight)\n",
        "print(\"Sentiment:\", final_sentiment)"
      ],
      "metadata": {
        "id": "TwAPOgK5nIDz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8088c38a-f32b-42bb-9b37-b4a1118dca6b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bobot: 0.5113766120260113\n",
            "Sentiment: PositiveðŸ“ˆ\n"
          ]
        }
      ]
    }
  ]
}