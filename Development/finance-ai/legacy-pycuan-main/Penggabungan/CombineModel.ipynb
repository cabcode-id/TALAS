{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JRNTF-yGVCey"
      },
      "outputs": [],
      "source": [
        "!pip install -q contractions scikit-learn Sastrawi googletrans==4.0.0-rc1 langdetect pandas matplotlib yfinance tensorflow xgboost\n",
        "\n",
        "# Import library\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import unicodedata\n",
        "import yfinance as yf\n",
        "import nltk\n",
        "import re\n",
        "import contractions\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "from sklearn.utils import resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from bs4 import BeautifulSoup\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory\n",
        "from wordcloud import WordCloud\n",
        "from collections import Counter\n",
        "from langdetect import detect\n",
        "from googletrans import Translator\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.preprocessing import MinMaxScaler, StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from math import sqrt\n",
        "from xgboost import XGBRegressor\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "from tensorflow import keras\n",
        "from math import sqrt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "mtVQ0e22z5Hl"
      },
      "outputs": [],
      "source": [
        "# Load  dataset\n",
        "url = 'https://raw.githubusercontent.com/22bayusetia/PyCuan/main/Sentiment%20Analysis/data_finance.csv'\n",
        "df = pd.read_csv(url, delimiter=',', encoding='latin-1', header=None)\n",
        "df = df.drop(0)\n",
        "df.columns = ['label', 'en_text', 'id_text']\n",
        "# df.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nGce2DZSVwJV",
        "outputId": "abd8a0e2-6834-4e2b-8322-6d9d0ad705d8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "<ipython-input-3-624d22d851f9>:12: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
            "  soup = BeautifulSoup(text, \"html.parser\")\n"
          ]
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "indonesian_stopwords = set(nltk.corpus.stopwords.words('indonesian'))\n",
        "factory1 = StopWordRemoverFactory()\n",
        "stopword_sastrawi = factory1.create_stop_word_remover()\n",
        "factory2 = StemmerFactory()\n",
        "stemmer_sastrawi = factory2.create_stemmer()\n",
        "\n",
        "# Fungsi preprocessing data\n",
        "def strip_html_tags(text):\n",
        "    soup = BeautifulSoup(text, \"html.parser\")\n",
        "    [s.extract() for s in soup(['iframe', 'script'])]\n",
        "    stripped_text = soup.get_text()\n",
        "    stripped_text = re.sub(r'[\\r|\\n|\\r\\n]+', '\\n', stripped_text)\n",
        "    return stripped_text\n",
        "\n",
        "def remove_accented_chars(text):\n",
        "    text = unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
        "    return text\n",
        "\n",
        "def preprocess_text_sastrawi(text):\n",
        "    # Melakukan preprocessing menggunakan Sastrawi\n",
        "    tokens = nltk.word_tokenize(text)\n",
        "    tokens = [stopword_sastrawi.remove(token) for token in tokens]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "def pre_process_text(text, language):\n",
        "    text = text.lower()\n",
        "    text = strip_html_tags(text)\n",
        "    text = text.translate(text.maketrans(\"\\n\\t\\r\", \"   \"))\n",
        "    text = remove_accented_chars(text)\n",
        "    text = contractions.fix(text)\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text, re.I | re.A)\n",
        "    text = re.sub(' +', ' ', text)\n",
        "    text = preprocess_text_sastrawi(text) if language == 'indonesian' else text\n",
        "    return text\n",
        "\n",
        "# Melakukan preprocessing pada data\n",
        "df['en_text'] = df['en_text'].apply(lambda x: pre_process_text(x, 'english'))\n",
        "df['id_text'] = df['id_text'].apply(lambda x: pre_process_text(x, 'indonesian'))\n",
        "df = df.drop_duplicates(subset=['en_text', 'id_text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "4ee2b4a_Zvo2"
      },
      "outputs": [],
      "source": [
        "def perform_sentiment_analysis(df, narrative=None):\n",
        "    # Upsample data\n",
        "    data_majority = df[df['label'] == \"positive\"]\n",
        "    data_minority = df[df['label'] == \"negative\"]\n",
        "\n",
        "    data_minority_upsampled = resample(data_minority,\n",
        "                                       replace=True,\n",
        "                                       n_samples=data_majority.shape[0],\n",
        "                                       random_state=123)\n",
        "\n",
        "    df_balance_upsampled = pd.concat([data_majority, data_minority_upsampled])\n",
        "    df_balanced_upsampled = df.drop_duplicates(subset=['en_text', 'id_text'])\n",
        "\n",
        "    X_train, X_temp, y_train, y_temp = train_test_split(df_balance_upsampled.en_text,\n",
        "                                                        df_balance_upsampled.label,\n",
        "                                                        test_size=0.2,\n",
        "                                                        random_state=42)\n",
        "\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "    # Konversi teks ke vektor fitur menggunakan TF-IDF\n",
        "    tfidf_vectorizer = TfidfVectorizer(max_features=3000)\n",
        "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
        "    X_val_tfidf = tfidf_vectorizer.transform(X_val)\n",
        "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
        "\n",
        "    rf_classifier = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "    rf_classifier.fit(X_train_tfidf, y_train)\n",
        "    y_pred = rf_classifier.predict(X_test_tfidf)\n",
        "\n",
        "    train_acc = rf_classifier.score(X_train_tfidf, y_train)\n",
        "    val_acc = rf_classifier.score(X_val_tfidf, y_val)\n",
        "    test_acc = rf_classifier.score(X_test_tfidf, y_test)\n",
        "\n",
        "    precision = precision_score(y_test, y_pred, average='weighted')\n",
        "    recall = recall_score(y_test, y_pred, average='weighted')\n",
        "    f1 = f1_score(y_test, y_pred, average='weighted')\n",
        "\n",
        "    predicted_sentiments = rf_classifier.predict(X_test_tfidf)\n",
        "    positive_percentage = (predicted_sentiments == 'positive').sum() / len(predicted_sentiments) * 100\n",
        "    negative_percentage = 100 - positive_percentage\n",
        "\n",
        "    # Deteksi bahasa narasi\n",
        "    if narrative:\n",
        "        translator = Translator()\n",
        "        language = detect(narrative)\n",
        "        if language != 'en':\n",
        "            narrative_translated = translator.translate(narrative, dest='en').text\n",
        "            narrative_tfidf = tfidf_vectorizer.transform([narrative_translated])\n",
        "        else:\n",
        "            narrative_tfidf = tfidf_vectorizer.transform([narrative])\n",
        "\n",
        "        predicted_sentiment = rf_classifier.predict(narrative_tfidf)\n",
        "        sentiment_probability = rf_classifier.predict_proba(narrative_tfidf)[0, 1]\n",
        "        threshold = 0.5  # Tresholdnya diatur sesuai kebutuhan\n",
        "        sentiment = \"Positive\" if sentiment_probability > threshold else \"Negative\"\n",
        "\n",
        "        print(\"Text:\", narrative)\n",
        "        print(\"Sentiment Probability:\", sentiment_probability)\n",
        "\n",
        "    return train_acc, val_acc, test_acc, precision, recall, f1, positive_percentage, negative_percentage, sentiment_probability"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "XUDLsRLZmCGU"
      },
      "outputs": [],
      "source": [
        "# Fungsi untuk melakukan prediksi time series\n",
        "def perform_time_series_forecasting(df, stock_symbol, start_date, end_date, seq_length=30, forecast_days=5):\n",
        "    df_stock = yf.download(stock_symbol, start=start_date, end=end_date)\n",
        "    ts = df_stock['Open'].values\n",
        "\n",
        "    scaler = StandardScaler()\n",
        "    ts_scaled = scaler.fit_transform(np.array(ts).reshape(-1, 1))\n",
        "\n",
        "    X_train, y_train = [], []\n",
        "\n",
        "    for i in range(len(ts_scaled) - seq_length):\n",
        "        X_train.append(ts_scaled[i:i + seq_length])\n",
        "        y_train.append(ts_scaled[i + seq_length])\n",
        "\n",
        "    X_train, y_train = np.array(X_train), np.array(y_train)\n",
        "\n",
        "    train_size = int(len(X_train) * 0.8)\n",
        "    X_train, X_test = X_train[:train_size], X_train[train_size:]\n",
        "    y_train, y_test = y_train[:train_size], y_train[train_size:]\n",
        "\n",
        "    # model LSTM\n",
        "    model = keras.Sequential([\n",
        "        LSTM(128, activation='relu', return_sequences=True, input_shape=(seq_length, 1)),\n",
        "        Dropout(0.2),\n",
        "        LSTM(128, activation='relu', return_sequences=True),\n",
        "        Dropout(0.2),\n",
        "        LSTM(128, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(64, activation='relu'),\n",
        "        Dropout(0.2),\n",
        "        Dense(1)\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
        "    model.fit(X_train, y_train, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping], verbose=0)\n",
        "\n",
        "    # Generate forecast for the next 'forecast_days'\n",
        "    X_forecast = np.copy(X_test[-1])\n",
        "    forecasted_values = []\n",
        "\n",
        "    for _ in range(forecast_days):\n",
        "        forecasted_value = model.predict(X_forecast.reshape(1, seq_length, 1))\n",
        "        forecasted_values.append(forecasted_value[0, 0])\n",
        "\n",
        "        X_forecast = np.roll(X_forecast, -1)\n",
        "        X_forecast[-1] = forecasted_value\n",
        "\n",
        "    forecasted_values = scaler.inverse_transform(np.array(forecasted_values).reshape(-1, 1))\n",
        "\n",
        "    # Evaluasi hasil prediksi\n",
        "    rmse_test = sqrt(mean_squared_error(y_test, model.predict(X_test)))\n",
        "    mae_test = mean_absolute_error(y_test, model.predict(X_test))\n",
        "\n",
        "    weighted_metric = (rmse_test + mae_test) / 2\n",
        "    last_date = df_stock.index[-1]\n",
        "    forecast_dates = pd.date_range(last_date, periods=forecast_days + 1)[1:]\n",
        "\n",
        "    last_actual_opening_price = df_stock['Open'][-1]\n",
        "    first_forecast_opening_price = forecasted_values[0][0]\n",
        "    price_difference = first_forecast_opening_price - last_actual_opening_price\n",
        "    percentage_change = price_difference / last_actual_opening_price\n",
        "    adjusted_percentage_change = (percentage_change + 1) / 2\n",
        "\n",
        "    return weighted_metric, forecast_dates, forecasted_values, adjusted_percentage_change\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9ZTc9YvvkN7c",
        "outputId": "b30feb8b-1276-4915-c2f1-1fb10f3371bb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*********************100%%**********************]  1 of 1 completed\n",
            "1/1 [==============================] - 2s 2s/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 126ms/step\n",
            "1/1 [==============================] - 0s 100ms/step\n",
            "1/1 [==============================] - 0s 57ms/step\n",
            "3/3 [==============================] - 0s 65ms/step\n",
            "3/3 [==============================] - 0s 52ms/step\n"
          ]
        }
      ],
      "source": [
        "def translate_to_english(narrative):\n",
        "    translator = Translator()\n",
        "    english_narrative = translator.translate(narrative, dest='en').text\n",
        "    return english_narrative\n",
        "\n",
        "# Input data testing\n",
        "narrative = \"Revisi Kebijakan Pupuk Bersubsidi, Kini Petani Dapat Tebus Pakai KTP\"  # lebih bagus pakai b. inggris\n",
        "\n",
        "stock_symbol = 'ETH-USD' # tambahkan .JK untuk bursa efek indonesia | -USD untuk global\n",
        "start_date = '2022-12-05'\n",
        "end_date = '2023-12-05'\n",
        "\n",
        "narrative_english = translate_to_english(narrative)\n",
        "time_series_results = perform_time_series_forecasting(df, stock_symbol, start_date, end_date)\n",
        "time_series_weight = time_series_results[-1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ozdzRc9zkUI2",
        "outputId": "2dd53860-ac7b-41b5-ceb8-e0a810282764"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: Revision of subsidized fertilizer policy, now farmers can redeem their KTP\n",
            "Sentiment Probability: 0.67\n",
            "\n",
            "Stock: ETH-USD\n",
            "Time Series Probability: 0.4357646897775186\n"
          ]
        }
      ],
      "source": [
        "sentiment_results = perform_sentiment_analysis(df, narrative=narrative_english)\n",
        "train_acc, val_acc, test_acc, precision, recall, f1, positive_percentage, negative_percentage, sentiment_probability = sentiment_results\n",
        "print(\"\\nStock:\", stock_symbol)\n",
        "print(\"Time Series Probability:\", time_series_weight)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "atvfe84c4Nri",
        "outputId": "f8510a77-72a8-453d-e863-613c11e40090"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bobot: 0.5880176414221315\n",
            "Sentiment: Positive 🚀\n"
          ]
        }
      ],
      "source": [
        "# Fungsi untuk mengkombinasikan bobot\n",
        "def combine_weights(sentiment_probability, time_series_weight, sentiment_ratio=0.65):\n",
        "    time_series_ratio = 1 - sentiment_ratio\n",
        "\n",
        "    combined_weight = (sentiment_ratio * sentiment_probability + time_series_ratio * time_series_weight)\n",
        "    return combined_weight\n",
        "\n",
        "final_weight = combine_weights(sentiment_probability, time_series_weight)\n",
        "final_sentiment = \"Positive 🚀\" if final_weight > 0.5 else \"Negative 🌂\"\n",
        "\n",
        "print(\"Bobot:\", final_weight)\n",
        "print(\"Sentiment:\", final_sentiment)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
