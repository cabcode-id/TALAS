{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8667a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import requests\n",
    "import re\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "# Tanggal yang dimasukkan ke csv. \n",
    "def get_raw_date():\n",
    "    return datetime.today().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# Membuat dictionary berisi judul dan link yang dibuat beberapa jam lalu atau kemarin. \n",
    "def find_links(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    divs = soup.find_all('div', class_='col-md-8')\n",
    "\n",
    "    span_elements = []\n",
    "\n",
    "    # Mengambil waktu saat ini dalam zona waktu Jakarta (GMT+7)\n",
    "    current_hour = datetime.now(timezone(timedelta(hours=7))).hour\n",
    "    # Menghitung jam yang sudah berlalu dari jam saat ini untuk crawl dari awal hari ini.\n",
    "    hour_patterns = [f\"\\\\b{i} jam lalu\\\\b\" for i in range(1, current_hour + 1)]\n",
    "    minute_pattern = \"\\\\b\\\\d+ menit lalu\\\\b\"\n",
    "    time_pattern = f\"{minute_pattern}|{('|'.join(hour_patterns))}\"\n",
    "\n",
    "    for div in divs:\n",
    "        spans = div.find_all('span', string=re.compile(time_pattern, re.IGNORECASE))\n",
    "        span_elements.extend(spans)\n",
    "\n",
    "    links_dict = {}\n",
    "    for span in span_elements:\n",
    "        parent = span.find_parent()\n",
    "        while parent:\n",
    "            link = parent.find('a', href=True)\n",
    "            if link:\n",
    "                title = link.get('title', '')\n",
    "                links_dict[title] = link['href']\n",
    "                break\n",
    "            parent = parent.find_parent()\n",
    "    return links_dict\n",
    "\n",
    "# Mengumpulkan artikel dari link yang ditemukan.\n",
    "def collect_articles(links_dict):\n",
    "    data = []\n",
    "    raw_date = get_raw_date()\n",
    "    source = \"Antara\"\n",
    "    for title, link in links_dict.items():\n",
    "        try:\n",
    "            get_content = requests.get(link, timeout=10)\n",
    "            get_content.raise_for_status()\n",
    "            date = raw_date\n",
    "            content, image_url = find_content(link)\n",
    "            # Process content directly\n",
    "            processed_content = extract_content(content)\n",
    "            data.append([title, source, link, image_url, date, processed_content])\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching content for {link}: {e}\")\n",
    "        time.sleep(2)\n",
    "    return data\n",
    "\n",
    "# Mencari konten dari link berita.\n",
    "def find_content(url):\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Find the image with the largest area instead of by class\n",
    "    image_url = \"\"\n",
    "    max_area = 0\n",
    "    for img in soup.find_all('img'):\n",
    "        # Skip small icons, emoticons etc\n",
    "        if not img.has_attr('src') or img['src'].startswith('data:'):\n",
    "            continue\n",
    "            \n",
    "        # Get width and height attributes\n",
    "        width = int(img.get('width', 0)) or int(re.search(r'width:\\s*(\\d+)px', img.get('style', '')) or [0, 0])[1]\n",
    "        height = int(img.get('height', 0)) or int(re.search(r'height:\\s*(\\d+)px', img.get('style', '')) or [0, 0])[1]\n",
    "        \n",
    "        # Calculate area\n",
    "        area = width * height\n",
    "        \n",
    "        # Update max area and image URL if this image is larger\n",
    "        if area > max_area:\n",
    "            max_area = area\n",
    "            image_url = img['src']\n",
    "    \n",
    "    # If no area detected images, fallback to traditional method\n",
    "    if not image_url:\n",
    "        image_div = soup.find('div', class_='wrap__article-detail-image mt-4')\n",
    "        if image_div:\n",
    "            img_tag = image_div.find('img', class_='img-fluid')\n",
    "            if img_tag and img_tag.has_attr('src'):\n",
    "                image_url = img_tag['src']\n",
    "    \n",
    "    parent_p_count = {}\n",
    "    # Mencari semua parent yang memiliki p. Jumlah p yang tertinggi = main content yang ingin diambil.\n",
    "    for p in soup.find_all('p'):\n",
    "        # Skip paragraphs that have any class attribute\n",
    "        if p.has_attr('class'):\n",
    "            continue\n",
    "        # Skip paragraphs that contain a span with class 'baca-juga'\n",
    "        if p.find('span', class_='baca-juga'):\n",
    "            continue\n",
    "\n",
    "        parent = p.find_parent()\n",
    "        if parent:\n",
    "            parent_p_count[parent] = parent_p_count.get(parent, 0) + 1\n",
    "\n",
    "    content_text = \"\"\n",
    "    if parent_p_count:\n",
    "        max_parent = max(parent_p_count, key=parent_p_count.get)\n",
    "        paragraphs = []\n",
    "        for p in max_parent.find_all('p'):\n",
    "            if p.has_attr('class'):\n",
    "                continue\n",
    "            if p.find('span', class_='baca-juga'):\n",
    "                continue\n",
    "            text = p.get_text(\" \", strip=True)\n",
    "            paragraphs.append(text)\n",
    "        content_text = \" \".join(paragraphs)\n",
    "    \n",
    "    return content_text, image_url\n",
    "\n",
    "def extract_content(text):\n",
    "    text = text.lower()\n",
    "    start_phrases = [\"jakarta (antara) - \"]\n",
    "    start_pattern = \"|\".join(map(re.escape, start_phrases))\n",
    "    pattern = f\"({start_pattern})(.*)\"\n",
    "    match = re.search(pattern, text, re.DOTALL)\n",
    "    return match.group(2).strip() if match else text\n",
    "\n",
    "def crawl_antara():\n",
    "    all_links = {}\n",
    "    page = 1\n",
    "    \n",
    "    while True:\n",
    "        url = f'https://www.antaranews.com/terkini/{page}'\n",
    "        try:\n",
    "            links_dict = find_links(url)\n",
    "            if not links_dict:\n",
    "                print(f\"No articles found on page {page}, stopping crawler\")\n",
    "                break\n",
    "            \n",
    "            all_links.update(links_dict)\n",
    "            print(f\"Found {len(links_dict)} articles on page {page}\")\n",
    "            page += 1\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error accessing page {page}: {e}\")\n",
    "            break\n",
    "    \n",
    "    # Process all collected links\n",
    "    data = collect_articles(all_links)\n",
    "    \n",
    "    # Format data for return\n",
    "    formatted_data = []\n",
    "    for row in data:\n",
    "        formatted_data.append({\n",
    "            'title': row[0],\n",
    "            'source': row[1],\n",
    "            'url': row[2],\n",
    "            'image': row[3],\n",
    "            'date': row[4],\n",
    "            'content': row[5]\n",
    "        })\n",
    "    \n",
    "    return formatted_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191be1be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15 articles on page 1\n",
      "Found 15 articles on page 2\n",
      "Found 15 articles on page 2\n",
      "Found 15 articles on page 3\n",
      "Found 15 articles on page 3\n",
      "Found 15 articles on page 4\n",
      "Found 15 articles on page 4\n",
      "Found 15 articles on page 5\n",
      "Found 15 articles on page 5\n",
      "Found 13 articles on page 6\n",
      "Found 13 articles on page 6\n",
      "Found 13 articles on page 7\n",
      "Found 13 articles on page 7\n",
      "Found 12 articles on page 8\n",
      "Found 12 articles on page 8\n",
      "Found 12 articles on page 9\n",
      "Found 12 articles on page 9\n",
      "Found 13 articles on page 10\n",
      "Found 13 articles on page 10\n",
      "Found 14 articles on page 11\n",
      "Found 14 articles on page 11\n",
      "Found 14 articles on page 12\n",
      "Found 14 articles on page 12\n",
      "Found 15 articles on page 13\n",
      "Found 15 articles on page 13\n",
      "Found 13 articles on page 14\n",
      "Found 13 articles on page 14\n",
      "Found 14 articles on page 15\n",
      "Found 14 articles on page 15\n",
      "Found 15 articles on page 16\n",
      "Found 15 articles on page 16\n",
      "Found 14 articles on page 17\n",
      "Found 14 articles on page 17\n",
      "Found 14 articles on page 18\n",
      "Found 14 articles on page 18\n",
      "Found 15 articles on page 19\n",
      "Found 15 articles on page 19\n",
      "Found 15 articles on page 20\n",
      "Found 15 articles on page 20\n",
      "Found 14 articles on page 21\n",
      "Found 14 articles on page 21\n",
      "Found 15 articles on page 22\n",
      "Found 15 articles on page 22\n",
      "Found 14 articles on page 23\n",
      "Found 14 articles on page 23\n",
      "Found 12 articles on page 24\n",
      "Found 12 articles on page 24\n",
      "Found 15 articles on page 25\n",
      "Found 15 articles on page 25\n",
      "Found 15 articles on page 26\n",
      "Found 15 articles on page 26\n",
      "Found 15 articles on page 27\n",
      "Found 15 articles on page 27\n",
      "Found 14 articles on page 28\n",
      "Found 14 articles on page 28\n",
      "Found 15 articles on page 29\n",
      "Found 15 articles on page 29\n",
      "Found 13 articles on page 30\n",
      "Found 13 articles on page 30\n",
      "Found 15 articles on page 31\n",
      "Found 15 articles on page 31\n",
      "Found 15 articles on page 32\n",
      "Found 15 articles on page 32\n",
      "Found 15 articles on page 33\n",
      "Found 15 articles on page 33\n",
      "Found 1 articles on page 34\n",
      "Found 1 articles on page 34\n",
      "No articles found on page 35, stopping crawler\n",
      "No articles found on page 35, stopping crawler\n"
     ]
    }
   ],
   "source": [
    "print(crawl_antara())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec8bfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone, timedelta\n",
    "\n",
    "current_hour = datetime.now(timezone(timedelta(hours=7))).hour\n",
    "hour_patterns = [f\"\\\\b{i} jam lalu\\\\b\" for i in range(1, current_hour + 1)]\n",
    "minute_pattern = \"\\\\b\\\\d+ menit lalu\\\\b\"\n",
    "time_pattern = f\"{minute_pattern}|{('|'.join(hour_patterns))}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "20e2b568",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'menit|1 jam|2 jam|3 jam|4 jam|5 jam|6 jam|7 jam|8 jam|9 jam|10 jam|11 jam|12 jam'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "time_pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3e7ac53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.antaranews.com/terkini/11'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7442d654",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\b\\d+ menit lalu\\b|\\b1 jam lalu\\b|\\b2 jam lalu\\b|\\b3 jam lalu\\b|\\b4 jam lalu\\b|\\b5 jam lalu\\b|\\b6 jam lalu\\b|\\b7 jam lalu\\b|\\b8 jam lalu\\b|\\b9 jam lalu\\b|\\b10 jam lalu\\b|\\b11 jam lalu\\b|\\b12 jam lalu\\b|\\b13 jam lalu\\b\n"
     ]
    }
   ],
   "source": [
    "response = requests.get(url)\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "divs = soup.find_all('div', class_='col-md-8')\n",
    "\n",
    "span_elements = []\n",
    "\n",
    "# Mengambil waktu saat ini dalam zona waktu Jakarta (GMT+7)\n",
    "current_hour = datetime.now(timezone(timedelta(hours=7))).hour\n",
    "# Menghitung jam yang sudah berlalu dari jam saat ini untuk crawl dari awal hari ini.\n",
    "hour_patterns = [f\"\\\\b{i} jam lalu\\\\b\" for i in range(1, current_hour + 1)]\n",
    "minute_pattern = \"\\\\b\\\\d+ menit lalu\\\\b\"\n",
    "time_pattern = f\"{minute_pattern}|{('|'.join(hour_patterns))}\"\n",
    "print(time_pattern)\n",
    "for div in divs:\n",
    "    spans = div.find_all('span', string=re.compile(time_pattern, re.IGNORECASE))\n",
    "    span_elements.extend(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5d45e97b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<span class=\"text-secondary\">13 jam lalu</span>,\n",
       " <span class=\"text-secondary\">13 jam lalu</span>,\n",
       " <span class=\"text-secondary\">13 jam lalu</span>,\n",
       " <span class=\"text-secondary\">13 jam lalu</span>,\n",
       " <span class=\"text-secondary\">13 jam lalu</span>,\n",
       " <span class=\"text-secondary\">13 jam lalu</span>,\n",
       " <span class=\"text-secondary\">13 jam lalu</span>]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "span_elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6c96833",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
